{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### This file can be used to reproduce the Acadia related part in Table 2, 3, 7 and Table 4 in the paper. \nThis file reads data directly from Github. If you would like to use the Trip Construction file to generate your own output in Binder, simply change the *pmatrix_url* in Chunk[5] to the one under option 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -r requirements.txt","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: numpy==1.20.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.20.2)\nRequirement already satisfied: pandas==1.2.4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.2.4)\nRequirement already satisfied: patsy==0.5.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.5.1)\nRequirement already satisfied: scikit-learn==0.24.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.24.1)\nRequirement already satisfied: scipy==1.6.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.6.3)\nRequirement already satisfied: sklearn==0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (0.0)\nRequirement already satisfied: statsmodels==0.12.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.12.2)\nRequirement already satisfied: pytz>=2017.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas==1.2.4->-r requirements.txt (line 2)) (2021.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas==1.2.4->-r requirements.txt (line 2)) (2.8.1)\nRequirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.7/site-packages (from patsy==0.5.1->-r requirements.txt (line 3)) (1.15.0)\nRequirement already satisfied: joblib>=0.11 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from scikit-learn==0.24.1->-r requirements.txt (line 4)) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from scikit-learn==0.24.1->-r requirements.txt (line 4)) (2.1.0)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nimport scipy\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy import stats\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning) ","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_date(df):\n    df['datetaken'] = pd.to_datetime(df['datetaken'])\n    df['date'] = [d.date() for d in df['datetaken']]\n    df['year'] = pd.DatetimeIndex(df['date']).year\n    df['month'] = pd.DatetimeIndex(df['date']).month\n    return df\n\ndef subset_data(input,month):\n    subset = input[input['month'] == month]\n    return subset","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park = 'acadia'","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"https://raw.githubusercontent.com/meilinshi/Socially-aware-Huff-model/main/Data/\"\ninput_url = path+park+\"_NP_cluster.csv\"\nposition_url = path+park+\"_NP_coords.csv\"\ndist_matrix_url = path+park+\"_NP_dist_matrix.csv\"\n\n# load the Flickr photo with cluster information\ndf = pd.read_csv(input_url)\ndf = split_date(df) \n\n# load the position of attractions\nposition = pd.read_csv(position_url) \nposition['coord'] = list(zip(position.Latitude, position.Longitude))\nplaces = position['Clusters from Data'].values\n\n# load the distance matrix\ndist_matrix = pd.read_csv(dist_matrix_url,index_col=0)\n\n\n# load probability matrix for the observed trips in the park\n# option 1: read data directly from Github\n#pmatrix_url = path+park+\"_pmatrix/\"+park+\"_NP_cluster_prob_matrix_\"\n\n# option 2: read data from Binder when you generate your own output from the Trip Construction file\npmatrix_url = park+\"_NP_cluster_prob_matrix_\"\n\npmatrix_all = [pmatrix_url+str(i)+\".csv\" for i in range(1,13)]\npmatrix_summer = [pmatrix_url+str(i)+\".csv\" for i in range(5,10)]\npmatrix_non_summer = [pmatrix_url+str(i)+\".csv\" for i in range(1,5)]+[pmatrix_url+str(i)+\".csv\" for i in range(10,13)]","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Construct distance matrix and attractiveness matrix needed for the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate travel distance (in km) using google map distance matrix api\n# import googlemaps\n# API_key = 'xxxxx'\n# gmaps = googlemaps.Client(key=API_key)\n\ndef get_dist_matrix(df):\n    destinations = df.coord\n    names = df['Clusters from Data'].values    \n    dim = len(destinations)\n    dist_matrix = np.zeros((dim, dim), float)        \n    for i in range(dim):\n        actual_distance = []\n        origin = destinations[i]        \n        for destination in destinations:\n            result = gmaps.distance_matrix(origin, destination, mode='driving')['rows'][0]['elements'][0]['distance']['value']\n            result = result/1000\n            actual_distance.append(result)\n        dist_matrix[i] = actual_distance       \n    res = pd.DataFrame(data=dist_matrix, index = names, columns=names)\n    return res\n\n# generate a distance matrix by distance matrix API\n# dist_matrix = get_dist_matrix(position)\n# Here a download version of distance matrix in the data folder is used.\n\n# generate attractiveness matrix, val indicates different measurements of attractiveness\ndef attr_matrix(df, month, val):\n    attr_matrix = pd.DataFrame()\n    df = subset_data(df, month)    \n    attr_matrix['Places'] = position['Clusters from Data'].values\n    attr_matrix['photo_views'] = df.groupby(['Cluster'])['views'].agg('sum')\n    attr_matrix['num_uploaders'] = df.groupby(['Cluster'])['owner'].nunique()\n    attr_matrix['num_of_photos'] = df.groupby(['Cluster']).size()\n    attr_matrix['avg_view_per_user'] = attr_matrix['photo_views']/attr_matrix['num_uploaders']\n    if val == 1:\n        attr_matrix['total_attr'] = attr_matrix['num_of_photos'] #Aj1\n    if val == 2:\n        attr_matrix['total_attr'] = attr_matrix['num_uploaders'] #Aj2\n    if val == 3:\n        attr_matrix['total_attr'] = attr_matrix['num_of_photos'] * attr_matrix['avg_view_per_user'] #Aj3  \n    attr_matrix = attr_matrix.fillna(0)\n    attr_matrix['total_attr_log'] = np.log(attr_matrix['total_attr']+1)\n    attr_matrix = attr_matrix.set_index('Places')\n    return attr_matrix\n\n# generate attractiveness matrix with num_of_photos*avg_view_per_user, Aj3, without temporal factor\ndef attr_matrix_all(df):\n    attr_matrix = pd.DataFrame() \n    attr_matrix['Places'] = position['Clusters from Data'].values\n    attr_matrix['photo_views'] = df.groupby(['Cluster'])['views'].agg('sum')\n    attr_matrix['num_uploaders'] = df.groupby(['Cluster'])['owner'].nunique()\n    attr_matrix['num_of_photos'] = df.groupby(['Cluster']).size()\n    attr_matrix['avg_view_per_user'] = attr_matrix['photo_views']/attr_matrix['num_uploaders']\n    attr_matrix['total_attr'] = attr_matrix['num_of_photos'] * attr_matrix['avg_view_per_user']\n    attr_matrix['total_attr_log'] = np.log(attr_matrix['total_attr'])\n    attr_matrix = attr_matrix.set_index('Places')\n    return attr_matrix\n\n\n# to include the neighboring effect\n# select K neighbors\ndef neighbors(dest, dist_matrix, K):\n    destinations = dist_matrix.index.values\n    dist_tp = np.transpose(dist_matrix)\n    neighbors = dist_tp.nsmallest(10, [dest])[1:K+1].index.values   \n    return neighbors\n\n\n# calculate centrality score based on K neighbors, attraction matrix and distance matrix\ndef centrality(dest, attr_matrix, K):\n    neighbor_lst = neighbors(dest, dist_matrix, K)\n    c = 0\n    dist = 0\n    for p in neighbor_lst:\n        c += attr_matrix.loc[p]['total_attr_log']/dist_matrix.loc[dest][p]\n        dist += dist_matrix.loc[dest][p]\n        c = c/dist\n    return c","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ordinary Least Squares (OLS) Calibration"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getComplement(item, lst):\n    results = []\n    for num in lst:\n        if num != item: \n            results.append(num)\n    return results\n\n# OLS dependent variable\ndef read_actual(pmatrix, origin):\n    num = 0\n    denom = 0\n    result = []\n    places = position['Clusters from Data'].values\n    dests = getComplement(origin, places)   \n    actual_pmatrix = pd.read_csv(pmatrix, index_col=0)\n    for i in range(len(dests)):\n        num = actual_pmatrix.loc[origin].values[i]\n        denom = np.mean(actual_pmatrix.loc[origin])\n        result.append(num/denom)\n    return result\n\n# OLS independent variables\n# attractiveness (including Social Influence), distance, centrality, without temporal factor\ndef log_transform_x_NT(origin,K):\n    X1, X2, X3 = [],[],[]\n    total_centrality = 0\n    places = position['Clusters from Data'].values\n    dests = getComplement(origin, places)\n    attr_mat = attr_matrix_all(df)     \n    for dest in dests:\n        total_centrality += centrality(dest, attr_mat, K)\n        X1.append(attr_mat.loc[dest]['total_attr_log']/np.mean(attr_mat['total_attr_log']))\n        X2.append(dist_matrix.loc[origin][dest]/ np.mean(dist_matrix.loc[origin]))\n        X3.append(centrality(dest, attr_mat, K)/(total_centrality/len(dests)))\n    var_table = pd.DataFrame()\n    X1 = [x + 1 for x in X1]\n    X3 = [x + 1 for x in X3]\n    var_table['x1'] = np.nan_to_num(np.log(X1))\n    var_table['x2'] = np.nan_to_num(np.log(X2))\n    var_table['x3'] = np.nan_to_num(np.log(X3))\n    return var_table\n\n# OLS independent variables\n# val indicates different measurements of attractiveness\ndef log_transform_x(origin,K,month,val):\n    X1, X2, X3 = [],[],[]\n    total_centrality = 0\n    places = position['Clusters from Data'].values\n    dests = getComplement(origin, places)    \n    attr_mat = attr_matrix(df, month, val)\n    for dest in dests:\n        total_centrality += centrality(dest, attr_mat, K)\n        X1.append(attr_mat.loc[dest]['total_attr_log']/np.mean(attr_mat['total_attr_log']))\n        X2.append(dist_matrix.loc[origin][dest]/ np.mean(dist_matrix.loc[origin]))\n        X3.append(centrality(dest, attr_mat, K)/(total_centrality/len(dests)))\n    var_table = pd.DataFrame()\n    X1 = [x + 1 for x in X1]\n    X3 = [x + 1 for x in X3]\n    var_table['x1'] = np.nan_to_num(np.log(X1))\n    var_table['x2'] = np.nan_to_num(np.log(X2))\n    var_table['x3'] = np.nan_to_num(np.log(X3))\n    return var_table","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Reproduce Table 2 in the paper"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y value used for both table 2 and 3\nY_res = []\nfor place in places:\n    for file in pmatrix_all:\n        Y = read_actual(file, place)\n        log_Y = np.nan_to_num(np.log(Y))\n        Y_res = np.append(Y_res, np.round(log_Y,10))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def var_tbl(val):\n    var_table = []\n    for place in places:\n        for i in range(1,13):\n            tbl = log_transform_x(place,2,i,val)\n            var_table.append(tbl)\n    df_var_tbl = pd.concat(var_table)\n    return df_var_tbl\n\ndef clear_var_tbl(df):\n    df['Y'] = Y_res\n    df = df[df.Y > 0]\n    df = df[df.x1 != 0]\n    return df\n\ndef return_tbl2_results(df):\n    df = clear_var_tbl(df)\n    X = df[['x1', 'x2','x3']]\n    Y = df['Y']\n    model = sm.OLS(Y,X).fit()\n    r2 = round(model.rsquared,3)\n    aic = round(model.aic,1)\n    return [r2,aic]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tbl2_results=[]\nfor val in range(1,4):\n    df_tbl2 = var_tbl(val)\n    tbl2_results.append(return_tbl2_results(df_tbl2))\n    \ndef generate_tbl2(tbl2_results):\n    df_tbl2 = pd.DataFrame(columns=['Aj1','Aj2','Aj3'])\n    df_tbl2['Aj1'] = tbl2_results[0]\n    df_tbl2['Aj2'] = tbl2_results[1]\n    df_tbl2['Aj3'] = tbl2_results[2]\n    df_tbl2 = df_tbl2.T\n    df_tbl2.columns =['R2', 'AIC']\n    df_tbl2['delta_AIC'] = df_tbl2['AIC'] - min(df_tbl2['AIC'])\n    df_tbl2['w_i'] = np.exp(-0.5*df_tbl2['delta_AIC'])/sum(np.exp(-0.5*df_tbl2['delta_AIC']))\n    return df_tbl2\n\npark_tbl2 = generate_tbl2(tbl2_results)\npark_tbl2","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"        R2    AIC  delta_AIC       w_i\nAj1  0.743  724.6       14.9  0.000581\nAj2  0.741  728.1       18.4  0.000101\nAj3  0.753  709.7        0.0  0.999318","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R2</th>\n      <th>AIC</th>\n      <th>delta_AIC</th>\n      <th>w_i</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Aj1</th>\n      <td>0.743</td>\n      <td>724.6</td>\n      <td>14.9</td>\n      <td>0.000581</td>\n    </tr>\n    <tr>\n      <th>Aj2</th>\n      <td>0.741</td>\n      <td>728.1</td>\n      <td>18.4</td>\n      <td>0.000101</td>\n    </tr>\n    <tr>\n      <th>Aj3</th>\n      <td>0.753</td>\n      <td>709.7</td>\n      <td>0.0</td>\n      <td>0.999318</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Reproduce Table 3 in the paper"},{"metadata":{"trusted":true},"cell_type":"code","source":"def var_tbl3(val, time):\n    var_table = []\n    for place in places:\n        for i in range(1,13):\n            if time == True:\n                tbl = log_transform_x(place,2,i,val)\n            else:\n                tbl = log_transform_x_NT(place,2)\n            var_table.append(tbl)\n    df_var_tbl = pd.concat(var_table)\n    return df_var_tbl\n\ndef return_tbl3_results(df, neighbor):\n    df = clear_var_tbl(df)\n    if neighbor == True:\n        X = df[['x1', 'x2','x3']]\n    else:\n        X = df[['x1', 'x2']]\n    Y = df['Y']\n    model = sm.OLS(Y,X).fit()\n    r2 = round(model.rsquared,3)\n    aic = round(model.aic,1)\n    return [r2,aic]","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_selection(neighbor, time):\n    df_tbl3 = var_tbl3(3,time) #Aj3 is used here\n    return return_tbl3_results(df_tbl3, neighbor)\n    \ndef generate_tbl3():\n    df_tbl3 = pd.DataFrame(columns=['SA_model','SA_model_no_N','SA_model_no_T','Huff_model'])\n    df_tbl3['SA_model'] = model_selection(neighbor=True, time=True)\n    df_tbl3['SA_model_no_N'] = model_selection(neighbor=False, time=True)\n    df_tbl3['SA_model_no_T'] = model_selection(neighbor=True, time=False)\n    df_tbl3['Huff_model'] = model_selection(neighbor=False, time=False)\n    df_tbl3 = df_tbl3.T\n    df_tbl3.columns =['R2', 'AIC']\n    df_tbl3['delta_AIC'] = df_tbl3['AIC'] - min(df_tbl3['AIC'])\n    df_tbl3['w_i'] = np.exp(-0.5*df_tbl3['delta_AIC'])/sum(np.exp(-0.5*df_tbl3['delta_AIC']))\n    return df_tbl3\n\npark_tbl3 = generate_tbl3()\npark_tbl3","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"                  R2    AIC  delta_AIC           w_i\nSA_model       0.753  709.7        0.0  9.859364e-01\nSA_model_no_N  0.746  718.2        8.5  1.406363e-02\nSA_model_no_T  0.744  748.5       38.8  3.702848e-09\nHuff_model     0.738  755.8       46.1  9.624121e-11","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R2</th>\n      <th>AIC</th>\n      <th>delta_AIC</th>\n      <th>w_i</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>SA_model</th>\n      <td>0.753</td>\n      <td>709.7</td>\n      <td>0.0</td>\n      <td>9.859364e-01</td>\n    </tr>\n    <tr>\n      <th>SA_model_no_N</th>\n      <td>0.746</td>\n      <td>718.2</td>\n      <td>8.5</td>\n      <td>1.406363e-02</td>\n    </tr>\n    <tr>\n      <th>SA_model_no_T</th>\n      <td>0.744</td>\n      <td>748.5</td>\n      <td>38.8</td>\n      <td>3.702848e-09</td>\n    </tr>\n    <tr>\n      <th>Huff_model</th>\n      <td>0.738</td>\n      <td>755.8</td>\n      <td>46.1</td>\n      <td>9.624121e-11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Reproduce Table 4 in the paper"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Y_res_place(place):\n    Y_res = []\n    for file in pmatrix_all:\n        Y = read_actual(file, place)\n        log_Y = np.nan_to_num(np.log(Y))\n        Y_res = np.append(Y_res, np.round(log_Y,10))\n    return Y_res\n\ndef var_table_place(place):\n    var_table = []\n    for i in range(1,13):\n        tbl = log_transform_x(place,2,i,3) #Aj3\n        var_table.append(tbl)\n    return pd.concat(var_table)\n\ndef clear_var_tbl_place(df, place):\n    df['Y'] = Y_res_place(place)\n    df = df[df.Y > 0]\n    df = df[df.x1 != 0]\n    return df\n\ndef return_tbl_4_5_results(df,place):\n    df = clear_var_tbl_place(df,place)\n    num_obs = len(df)\n    X = df[['x1', 'x2','x3']]\n    Y = df['Y']    \n    model = sm.OLS(Y,X).fit()    \n    alpha = round(model.params[0],4)\n    alpha_p = round(model.pvalues[0],3)\n    beta = round(model.params[1],4)\n    beta_p = round(model.pvalues[1],3)\n    theta = round(model.params[2],4)\n    theta_p = round(model.pvalues[2],3)\n    mse = round(model.mse_resid,3)\n    r2 = round(model.rsquared,3)    \n    return [place, num_obs, alpha, alpha_p, beta, beta_p, theta, theta_p, mse, r2]\n\ndef mark_significance(lst):\n    p_val = []\n    for item in lst:\n        if item <= 0.001:\n            p_val.append('***')\n        elif item > 0.001 and item <= 0.01:\n            p_val.append('**')\n        elif item > 0.01 and item <= 0.05:\n            p_val.append('*')\n        else:\n            p_val.append(' ')\n    return p_val","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tbl_4_5_results = []\nfor place in places:\n    var_table = var_table_place(place)\n    tbl_4_5_results.append(return_tbl_4_5_results(var_table,place))\n\ndef generate_tbl_4_5(tbl_results):\n    df_tbl_4_5 = pd.DataFrame(data=tbl_results,columns=['Place','num_obs','alpha','alpha_p','beta','beta_p','theta','theta_p','MSE','R2'])\n    df_tbl_4_5 = df_tbl_4_5[df_tbl_4_5['num_obs'] >= 30]\n    df_tbl_4_5['alpha_sig'] = mark_significance(df_tbl_4_5['alpha_p'])\n    df_tbl_4_5['beta_sig'] = mark_significance(df_tbl_4_5['beta_p'])\n    df_tbl_4_5['theta_sig'] = mark_significance(df_tbl_4_5['theta_p'])\n    df_tbl_4_5 = df_tbl_4_5[['Place','alpha','alpha_sig','beta','beta_sig','theta','theta_sig','MSE','R2']]\n    return df_tbl_4_5\n\ngenerate_tbl_4_5(tbl_4_5_results)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"                Place   alpha alpha_sig    beta beta_sig   theta theta_sig  \\\n1         Bass Harbor  0.8863         * -0.8608        *  0.1155             \n3    Northeast Harbor  0.1684           -0.1137           0.4079         *   \n4          Bar Harbor  1.3226       ***  0.2359          -0.0224             \n6   Cadillac Mountain  0.6601           -0.0218           0.1907             \n8         Bubble Rock  0.1722           -0.4898        *  0.3994         *   \n9         Jordan Pond  1.5456       *** -0.0670          -0.0133             \n10      Boulder Beach  2.0496       ***  0.3590        * -0.3446             \n11       Thunder Hole  1.2109        ** -0.1355           0.0232             \n12         Sand Beach  2.1061       ***  0.0374          -0.3318             \n\n      MSE     R2  \n1   0.273  0.731  \n3   0.208  0.838  \n4   0.322  0.739  \n6   0.360  0.707  \n8   0.322  0.791  \n9   0.203  0.886  \n10  0.334  0.751  \n11  0.301  0.782  \n12  0.397  0.742  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Place</th>\n      <th>alpha</th>\n      <th>alpha_sig</th>\n      <th>beta</th>\n      <th>beta_sig</th>\n      <th>theta</th>\n      <th>theta_sig</th>\n      <th>MSE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Bass Harbor</td>\n      <td>0.8863</td>\n      <td>*</td>\n      <td>-0.8608</td>\n      <td>*</td>\n      <td>0.1155</td>\n      <td></td>\n      <td>0.273</td>\n      <td>0.731</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Northeast Harbor</td>\n      <td>0.1684</td>\n      <td></td>\n      <td>-0.1137</td>\n      <td></td>\n      <td>0.4079</td>\n      <td>*</td>\n      <td>0.208</td>\n      <td>0.838</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bar Harbor</td>\n      <td>1.3226</td>\n      <td>***</td>\n      <td>0.2359</td>\n      <td></td>\n      <td>-0.0224</td>\n      <td></td>\n      <td>0.322</td>\n      <td>0.739</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Cadillac Mountain</td>\n      <td>0.6601</td>\n      <td></td>\n      <td>-0.0218</td>\n      <td></td>\n      <td>0.1907</td>\n      <td></td>\n      <td>0.360</td>\n      <td>0.707</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Bubble Rock</td>\n      <td>0.1722</td>\n      <td></td>\n      <td>-0.4898</td>\n      <td>*</td>\n      <td>0.3994</td>\n      <td>*</td>\n      <td>0.322</td>\n      <td>0.791</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Jordan Pond</td>\n      <td>1.5456</td>\n      <td>***</td>\n      <td>-0.0670</td>\n      <td></td>\n      <td>-0.0133</td>\n      <td></td>\n      <td>0.203</td>\n      <td>0.886</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Boulder Beach</td>\n      <td>2.0496</td>\n      <td>***</td>\n      <td>0.3590</td>\n      <td>*</td>\n      <td>-0.3446</td>\n      <td></td>\n      <td>0.334</td>\n      <td>0.751</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Thunder Hole</td>\n      <td>1.2109</td>\n      <td>**</td>\n      <td>-0.1355</td>\n      <td></td>\n      <td>0.0232</td>\n      <td></td>\n      <td>0.301</td>\n      <td>0.782</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Sand Beach</td>\n      <td>2.1061</td>\n      <td>***</td>\n      <td>0.0374</td>\n      <td></td>\n      <td>-0.3318</td>\n      <td></td>\n      <td>0.397</td>\n      <td>0.742</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Reproduce Table 7 in the paper"},{"metadata":{"trusted":true},"cell_type":"code","source":"def var_tbl_7(K, time):\n    var_table = []\n    if time == 0: #all-time\n        month_range = range(1,13)\n    if time == 1: # summer\n        month_range = range(5,10)\n    if time == 2: #non-summer\n        month_range = [1,2,3,4,10,11,12]\n    for place in places:\n        for i in month_range:\n            tbl = log_transform_x(place,K,i,3) #Aj3\n            var_table.append(tbl)\n    df_var_tbl = pd.concat(var_table)\n    return df_var_tbl\n\ndef clear_var_tbl(df, Y_val):\n    df['Y'] = Y_val\n    df = df[df.Y > 0]\n    df = df[df.x1 != 0]\n    return df\n\ndef return_tbl7_results(df,Y_val):\n    df = clear_var_tbl(df,Y_val)\n    X = df[['x1', 'x2','x3']]\n    Y = df['Y']\n    model = sm.OLS(Y,X).fit()\n    mse = round(model.mse_resid,6)\n    r2 = round(model.rsquared,3)\n    return [mse,r2]","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_res_all,Y_res_summer,Y_res_non_summer = [],[],[]\nfor place in places:\n    for file in pmatrix_all:\n        Y = read_actual(file, place)\n        log_Y = np.nan_to_num(np.log(Y))\n        Y_res_all = np.append(Y_res_all, np.round(log_Y,10))\n    for file in pmatrix_summer:\n        Y = read_actual(file, place)\n        log_Y = np.nan_to_num(np.log(Y))\n        Y_res_summer = np.append(Y_res_summer, np.round(log_Y,10))\n    for file in pmatrix_non_summer:\n        Y = read_actual(file, place)\n        log_Y = np.nan_to_num(np.log(Y))\n        Y_res_non_summer = np.append(Y_res_non_summer, np.round(log_Y,10))","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tbl7_all,tbl7_summer,tbl7_non_summer,tbl7_results=[],[],[],[]\nfor K in [2,3,5]:\n    df_all = var_tbl_7(K,0)\n    df_summer = var_tbl_7(K,1)\n    df_non_summer = var_tbl_7(K,2)   \n    tbl7_all.append(return_tbl7_results(df_all,Y_res_all))\n    tbl7_summer.append(return_tbl7_results(df_summer,Y_res_summer))\n    tbl7_non_summer.append(return_tbl7_results(df_non_summer,Y_res_non_summer))    \ntbl7_results = tbl7_all+tbl7_summer+tbl7_non_summer\n\ndef generate_tbl7(results):\n    df_tbl7 = pd.DataFrame(data=results)\n    df_tbl7.columns =['MSE', 'R2']\n    df_tbl7['Time'] = [\"All_year\"]*3+['Summer']*3+['Non_summer']*3\n    df_tbl7['K'] = [2,3,5]*3\n    df_tbl7 = df_tbl7.set_index(['Time', 'K'])    \n    return df_tbl7\n\npark_tbl7 = generate_tbl7(tbl7_results)\npark_tbl7","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"                   MSE     R2\nTime       K                 \nAll_year   2  0.351958  0.753\n           3  0.355672  0.750\n           5  0.360020  0.747\nSummer     2  0.239118  0.780\n           3  0.241003  0.778\n           5  0.241888  0.777\nNon_summer 2  0.470965  0.766\n           3  0.479230  0.762\n           5  0.490113  0.757","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>MSE</th>\n      <th>R2</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th>K</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">All_year</th>\n      <th>2</th>\n      <td>0.351958</td>\n      <td>0.753</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.355672</td>\n      <td>0.750</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.360020</td>\n      <td>0.747</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">Summer</th>\n      <th>2</th>\n      <td>0.239118</td>\n      <td>0.780</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.241003</td>\n      <td>0.778</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.241888</td>\n      <td>0.777</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">Non_summer</th>\n      <th>2</th>\n      <td>0.470965</td>\n      <td>0.766</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.479230</td>\n      <td>0.762</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.490113</td>\n      <td>0.757</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}