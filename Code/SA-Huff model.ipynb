{"cells":[{"metadata":{},"cell_type":"markdown","source":"This file is used to reproduce Section 5 in the paper, Table 2-6.\n\nAcadia National Park is used here as an example."},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install -r requirements.txt","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting numpy==1.20.2\n  Using cached numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\nCollecting pandas==1.2.4\n  Downloading pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n\u001b[K     |████████████████████████████████| 9.9 MB 4.5 MB/s eta 0:00:01\n\u001b[?25hCollecting patsy==0.5.1\n  Downloading patsy-0.5.1-py2.py3-none-any.whl (231 kB)\n\u001b[K     |████████████████████████████████| 231 kB 50.6 MB/s eta 0:00:01\n\u001b[?25hCollecting scikit-learn==0.24.1\n  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n\u001b[K     |████████████████████████████████| 22.3 MB 63.5 MB/s eta 0:00:01\n\u001b[?25hCollecting scipy==1.6.3\n  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n\u001b[K     |████████████████████████████████| 27.4 MB 36.7 MB/s eta 0:00:01     |███████████████▊                | 13.5 MB 36.7 MB/s eta 0:00:01\n\u001b[?25hCollecting sklearn==0.0\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\nCollecting statsmodels==0.12.2\n  Downloading statsmodels-0.12.2-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n\u001b[K     |████████████████████████████████| 9.5 MB 48.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas==1.2.4->-r requirements.txt (line 2)) (2021.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas==1.2.4->-r requirements.txt (line 2)) (2.8.1)\nRequirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.7/site-packages (from patsy==0.5.1->-r requirements.txt (line 3)) (1.15.0)\nCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\nCollecting joblib>=0.11\n  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n\u001b[K     |████████████████████████████████| 303 kB 44.0 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: sklearn\n  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=6666af4853a622d5234c0d36bad7e7480ebc9df089ebc23b4119df64aa670fa4\n  Stored in directory: /home/jovyan/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\nSuccessfully built sklearn\nInstalling collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn, patsy, pandas, statsmodels, sklearn\nSuccessfully installed joblib-1.0.1 numpy-1.20.2 pandas-1.2.4 patsy-0.5.1 scikit-learn-0.24.1 scipy-1.6.3 sklearn-0.0 statsmodels-0.12.2 threadpoolctl-2.1.0\n","name":"stdout"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nimport scipy\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy import stats\nimport statsmodels.api as sm","execution_count":3,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def split_date(df):\n    df['datetaken'] = pd.to_datetime(df['datetaken'])\n    df['date'] = [d.date() for d in df['datetaken']]\n    df['year'] = pd.DatetimeIndex(df['date']).year\n    df['month'] = pd.DatetimeIndex(df['date']).month\n    return df\n\ndef subset_data(input,month):\n    subset = input[input['month'] == month]\n    return subset","execution_count":4,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"acadia_url = \"https://raw.githubusercontent.com/meilinshi/Socially-aware-Huff-model/main/Data/acadia_NP_cluster.csv\"\nposition_url = \"https://raw.githubusercontent.com/meilinshi/Socially-aware-Huff-model/main/Data/acadia_NP_coords.csv\"\n\nposition = pd.read_csv(position_url) \nposition['coord'] = list(zip(position.Latitude, position.Longitude))\n\ndf = pd.read_csv(acadia_url)\ndf = split_date(df) \ndf.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   index           id         owner           datetaken   latitude  longitude  \\\n0      0   8918787381  74212514@N04 2010-01-10 15:50:46  44.354492 -68.051204   \n1      1  29498596186  74212514@N04 2010-01-10 16:03:20  44.354492 -68.051204   \n2      2   8919396564  74212514@N04 2010-01-10 16:15:59  44.354492 -68.051204   \n3      3   8918780331  74212514@N04 2010-01-10 16:31:06  44.354492 -68.051204   \n4      4   8918778905  74212514@N04 2010-01-10 16:42:40  44.354492 -68.051204   \n\n                          title  accuracy  views  Cluster        date  year  \\\n0          Acadia National Park      12.0    793        0  2010-01-10  2010   \n1  Maine - Acadia National Park      12.0   5829        0  2010-01-10  2010   \n2                      DSC03484      12.0     55        0  2010-01-10  2010   \n3                      DSC03491      12.0     57        0  2010-01-10  2010   \n4                      DSC03498      12.0     67        0  2010-01-10  2010   \n\n   month  \n0      1  \n1      1  \n2      1  \n3      1  \n4      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>owner</th>\n      <th>datetaken</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>title</th>\n      <th>accuracy</th>\n      <th>views</th>\n      <th>Cluster</th>\n      <th>date</th>\n      <th>year</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>8918787381</td>\n      <td>74212514@N04</td>\n      <td>2010-01-10 15:50:46</td>\n      <td>44.354492</td>\n      <td>-68.051204</td>\n      <td>Acadia National Park</td>\n      <td>12.0</td>\n      <td>793</td>\n      <td>0</td>\n      <td>2010-01-10</td>\n      <td>2010</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>29498596186</td>\n      <td>74212514@N04</td>\n      <td>2010-01-10 16:03:20</td>\n      <td>44.354492</td>\n      <td>-68.051204</td>\n      <td>Maine - Acadia National Park</td>\n      <td>12.0</td>\n      <td>5829</td>\n      <td>0</td>\n      <td>2010-01-10</td>\n      <td>2010</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>8919396564</td>\n      <td>74212514@N04</td>\n      <td>2010-01-10 16:15:59</td>\n      <td>44.354492</td>\n      <td>-68.051204</td>\n      <td>DSC03484</td>\n      <td>12.0</td>\n      <td>55</td>\n      <td>0</td>\n      <td>2010-01-10</td>\n      <td>2010</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>8918780331</td>\n      <td>74212514@N04</td>\n      <td>2010-01-10 16:31:06</td>\n      <td>44.354492</td>\n      <td>-68.051204</td>\n      <td>DSC03491</td>\n      <td>12.0</td>\n      <td>57</td>\n      <td>0</td>\n      <td>2010-01-10</td>\n      <td>2010</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>8918778905</td>\n      <td>74212514@N04</td>\n      <td>2010-01-10 16:42:40</td>\n      <td>44.354492</td>\n      <td>-68.051204</td>\n      <td>DSC03498</td>\n      <td>12.0</td>\n      <td>67</td>\n      <td>0</td>\n      <td>2010-01-10</td>\n      <td>2010</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Construct distance matrix and attractiveness matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Calculate travel distance (in km) using google map distance matrix api\n# import googlemaps\n# API_key = 'xxxxx'\n# gmaps = googlemaps.Client(key=API_key)\n\ndef get_dist_matrix(df):\n\n    destinations = df.coord\n    names = df['Clusters from Data'].values\n    \n    dim = len(destinations)\n    dist_matrix = np.zeros((dim, dim), float)\n    \n    \n    for i in range(dim):\n        actual_distance = []\n        origin = destinations[i]        \n        for destination in destinations:\n            result = gmaps.distance_matrix(origin, destination, mode='driving')['rows'][0]['elements'][0]['distance']['value']\n            result = result/1000\n            actual_distance.append(result)\n        dist_matrix[i] = actual_distance\n        \n    res = pd.DataFrame(data=dist_matrix, index = names, columns=names)\n    return res\n\n\n# generate attractiveness matrix\ndef attr_matrix(df, month):\n    attr_matrix = pd.DataFrame()\n    df = subset_data(df, month)\n    \n    attr_matrix['Places'] = position['Clusters from Data'].values\n    attr_matrix['photo_views'] = df.groupby(['Cluster'])['views'].agg('sum')\n    attr_matrix['num_uploaders'] = df.groupby(['Cluster'])['owner'].nunique()\n    attr_matrix['num_of_photos'] = df.groupby(['Cluster']).size()\n    attr_matrix['avg_view_per_user'] = attr_matrix['photo_views']/attr_matrix['num_uploaders']\n    attr_matrix['avg_view_per_photo'] = attr_matrix['photo_views']/attr_matrix['num_of_photos']\n    \n    # Different measurements of attractiveness in Section 5.1.2 and Table 2\n    attr_matrix['total_attr'] = attr_matrix['num_of_photos'] * attr_matrix['avg_view_per_user']\n    #attr_matrix['total_attr'] = attr_matrix['num_of_photos']\n    #attr_matrix['total_attr'] = attr_matrix['num_uploaders']\n    \n    attr_matrix = attr_matrix.fillna(0)\n    attr_matrix['total_attr_log'] = np.log(attr_matrix['total_attr']+1)\n    attr_matrix = attr_matrix.set_index('Places')\n    return attr_matrix\n\n\n# attractiveness without temporal component, used for model comparison in Section 5.1.3 and Table 3\ndef attr_matrix_all(df):\n    attr_matrix = pd.DataFrame() \n    attr_matrix['Places'] = position['Clusters from Data'].values\n    attr_matrix['photo_views'] = df.groupby(['Cluster'])['views'].agg('sum')\n    attr_matrix['num_uploaders'] = df.groupby(['Cluster'])['owner'].nunique()\n    attr_matrix['num_of_photos'] = df.groupby(['Cluster']).size()\n    attr_matrix['avg_view_per_user'] = attr_matrix['photo_views']/attr_matrix['num_uploaders']\n    attr_matrix['avg_view_per_photo'] = attr_matrix['photo_views']/attr_matrix['num_of_photos']\n    attr_matrix['total_attr'] = attr_matrix['num_of_photos'] * attr_matrix['avg_view_per_user']\n    attr_matrix['total_attr_log'] = np.log(attr_matrix['total_attr'])\n    attr_matrix = attr_matrix.set_index('Places')\n    return attr_matrix\n\n# to include the neighboring effect\n# select K neighbors\ndef neighbors(dest, dist_matrix, K):\n    destinations = dist_matrix.index.values\n    dist_tp = np.transpose(dist_matrix)\n    neighbors = dist_tp.nsmallest(10, [dest])[1:K+1].index.values   \n    return neighbors\n\n# calculate centrality score based on K neighbors, attraction matrix and distance matrix\ndef centrality(dest, attr_matrix, K):\n    neighbor_lst = neighbors(dest, dist_matrix, K)\n    c = 0\n    dist = 0\n    for p in neighbor_lst:\n        c += attr_matrix.loc[p]['total_attr_log']/dist_matrix.loc[dest][p]\n        dist += dist_matrix.loc[dest][p]\n        c = c/dist\n    return c","execution_count":4,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# generate a distance matrix by distance matrix API\n# dist_matrix = get_dist_matrix(position)\n\n# Here a download version of distance matrix is used.\ndist_matrix_url = \"https://raw.githubusercontent.com/meilinshi/Socially-aware-Huff-model/main/Data/acadia_NP_dist_matrix.csv\"\ndist_matrix = pd.read_csv(dist_matrix_url,index_col=0)\ndist_matrix.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"                    Schoodic Institute  Bass Harbor  Southwest Harbor  \\\nSchoodic Institute               0.000       81.026            74.132   \nBass Harbor                     84.206        0.000             8.080   \nSouthwest Harbor                77.312        8.080             0.000   \nNortheast Harbor                76.562       26.813            19.919   \nBar Harbor                      76.747       29.791            22.898   \n\n                    Northeast Harbor  Bar Harbor  Wild Gardens of Acadia  \\\nSchoodic Institute            73.382      73.512                  77.424   \nBass Harbor                   26.813      29.721                  32.390   \nSouthwest Harbor              19.919      22.828                  25.497   \nNortheast Harbor               0.000      17.660                  14.353   \nBar Harbor                    17.730       0.000                   4.416   \n\n                    Cadillac Mountain  Penobscot Peak  Bubble Rock  \\\nSchoodic Institute             84.215          69.457       82.548   \nBass Harbor                    34.466          22.888       32.798   \nSouthwest Harbor               27.572          15.994       25.905   \nNortheast Harbor               22.404           3.925       20.737   \nBar Harbor                     10.117          13.805        8.450   \n\n                    Jordan Pond  Boulder Beach  Thunder Hole  Sand Beach  \nSchoodic Institute       85.141         82.609        83.429      84.342  \nBass Harbor              35.392         37.576        38.395      39.309  \nSouthwest Harbor         28.498         30.682        31.502      32.415  \nNortheast Harbor         23.330         14.331        15.151      16.064  \nBar Harbor               11.043          9.602        10.422      11.335  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Schoodic Institute</th>\n      <th>Bass Harbor</th>\n      <th>Southwest Harbor</th>\n      <th>Northeast Harbor</th>\n      <th>Bar Harbor</th>\n      <th>Wild Gardens of Acadia</th>\n      <th>Cadillac Mountain</th>\n      <th>Penobscot Peak</th>\n      <th>Bubble Rock</th>\n      <th>Jordan Pond</th>\n      <th>Boulder Beach</th>\n      <th>Thunder Hole</th>\n      <th>Sand Beach</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Schoodic Institute</th>\n      <td>0.000</td>\n      <td>81.026</td>\n      <td>74.132</td>\n      <td>73.382</td>\n      <td>73.512</td>\n      <td>77.424</td>\n      <td>84.215</td>\n      <td>69.457</td>\n      <td>82.548</td>\n      <td>85.141</td>\n      <td>82.609</td>\n      <td>83.429</td>\n      <td>84.342</td>\n    </tr>\n    <tr>\n      <th>Bass Harbor</th>\n      <td>84.206</td>\n      <td>0.000</td>\n      <td>8.080</td>\n      <td>26.813</td>\n      <td>29.721</td>\n      <td>32.390</td>\n      <td>34.466</td>\n      <td>22.888</td>\n      <td>32.798</td>\n      <td>35.392</td>\n      <td>37.576</td>\n      <td>38.395</td>\n      <td>39.309</td>\n    </tr>\n    <tr>\n      <th>Southwest Harbor</th>\n      <td>77.312</td>\n      <td>8.080</td>\n      <td>0.000</td>\n      <td>19.919</td>\n      <td>22.828</td>\n      <td>25.497</td>\n      <td>27.572</td>\n      <td>15.994</td>\n      <td>25.905</td>\n      <td>28.498</td>\n      <td>30.682</td>\n      <td>31.502</td>\n      <td>32.415</td>\n    </tr>\n    <tr>\n      <th>Northeast Harbor</th>\n      <td>76.562</td>\n      <td>26.813</td>\n      <td>19.919</td>\n      <td>0.000</td>\n      <td>17.660</td>\n      <td>14.353</td>\n      <td>22.404</td>\n      <td>3.925</td>\n      <td>20.737</td>\n      <td>23.330</td>\n      <td>14.331</td>\n      <td>15.151</td>\n      <td>16.064</td>\n    </tr>\n    <tr>\n      <th>Bar Harbor</th>\n      <td>76.747</td>\n      <td>29.791</td>\n      <td>22.898</td>\n      <td>17.730</td>\n      <td>0.000</td>\n      <td>4.416</td>\n      <td>10.117</td>\n      <td>13.805</td>\n      <td>8.450</td>\n      <td>11.043</td>\n      <td>9.602</td>\n      <td>10.422</td>\n      <td>11.335</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Ordinary Least Squares (OLS) Calibration"},{"metadata":{"trusted":false},"cell_type":"code","source":"def getComplement(item, lst):\n    results = []\n    for num in lst:\n        if num != item: \n            results.append(num)\n    return results\n\n# OLS dependent variable\ndef read_actual(pmatrix, origin):\n    num = 0\n    denom = 0\n    result = []\n    places = position['Clusters from Data'].values\n    dests = getComplement(origin, places)   \n    actual_pmatrix = pd.read_csv(pmatrix, index_col=0)\n    for i in range(len(dests)):\n        num = actual_pmatrix.loc[origin].values[i]\n        denom = np.mean(actual_pmatrix.loc[origin])\n        result.append(num/denom)\n    return result\n\n# OLS independent variables\n# attractiveness (including Social Influence), distance, centrality\ndef log_transform_x(origin,K,month):\n    X1, X2, X3 = [],[],[]\n    total_centrality = 0\n    places = position['Clusters from Data'].values\n    dests = getComplement(origin, places)\n    attr_mat = attr_matrix(df, month)\n    #attr_mat = attr_matrix_all(df) # attr_matrix without temporal component, used for model comparison in Section 5.1.3 and Table 3\n    for dest in dests:\n        total_centrality += centrality(dest, attr_mat, K)\n        X1.append(attr_mat.loc[dest]['total_attr_log']/np.mean(attr_mat['total_attr_log']))\n        X2.append(dist_matrix.loc[origin][dest]/ np.mean(dist_matrix.loc[origin]))\n        X3.append(centrality(dest, attr_mat, K)/(total_centrality/len(dests)))\n    var_table = pd.DataFrame()\n    X1 = [x + 1 for x in X1]\n    X3 = [x + 1 for x in X3]\n    var_table['x1'] = np.nan_to_num(np.log(X1))\n    var_table['x2'] = np.nan_to_num(np.log(X2))\n    var_table['x3'] = np.nan_to_num(np.log(X3))\n    return var_table","execution_count":6,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# acadia probability matrix example\nplaces = position['Clusters from Data'].values\n\nacadia_pmatrix_url = \"https://raw.githubusercontent.com/meilinshi/Socially-aware-Huff-model/main/Data/acadia_pmatrix_example/acadia_NP_cluster_prob_matrix_\"\npmatrix_lst = [acadia_pmatrix_url+str(i)+\".csv\" for i in range(1,13)]","execution_count":21,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# for all trips in park (Section 5.1.2 and 5.1.3, Table 2 and 3)\nY_res = []\nfor place in places:\n    for file in pmatrix_lst:\n        Y = read_actual(file, place)\n        log_Y = np.nan_to_num(np.log(Y))\n        Y_res = np.append(Y_res, np.round(log_Y,10)) #reading from url seems to have a rounding issue (just in case)\n        \n        \n# for place related trips in park (Section 5.2 and 5.3, Table 4, 5, 6)\n# Y_res = []\n# place_name = places[7] # pick a specific place\n# for file in pmatrix_lst:\n    # Y = read_actual(file, place_name)\n    # log_Y = np.nan_to_num(np.log(Y))\n    # Y_res = np.append(Y_res, np.round(log_Y,10))","execution_count":46,"outputs":[{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n  \n/srv/conda/envs/notebook/lib/python3.7/site-packages/numpy/core/fromnumeric.py:58: RuntimeWarning: overflow encountered in multiply\n  return bound(*args, **kwds)\n/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n","name":"stderr"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"var_table = []\nfor place in places:\n    for i in range(1,13): #The range here can be changed to summer/winter months (Section 5.3 and Table 6)\n        tbl = log_transform_x(place,2,i)\n        var_table.append(tbl)\ndf_allmonth = pd.concat(var_table)","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### An example of all trips in Acadia National Park,  $R^2$ = 0.753"},{"metadata":{"trusted":false},"cell_type":"code","source":"## fit a OLS model on the three parameters\ndf_allmonth['Y'] = Y_res\ndf_allmonth = df_allmonth[df_allmonth.Y > 0]\ndf_allmonth = df_allmonth[df_allmonth.x1 != 0]\n\n#X = df_allmonth[['x1', 'x2']] # for model comparison in Section 5.1.3 and Table 3\nX = df_allmonth[['x1', 'x2','x3']]\nY = df_allmonth['Y']\n\nresults = sm.OLS(Y,X).fit()\nprint('Parameters: ', results.params)\nprint('R2: ', results.rsquared)\nprint('MSE:', results.mse_resid)\nprint('AIC: ', results.aic)","execution_count":48,"outputs":[{"output_type":"stream","text":"Parameters:  x1    1.019533\nx2   -0.107049\nx3    0.145088\ndtype: float64\nR2:  0.7528155298087753\nAIC:  709.6799332827493\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}